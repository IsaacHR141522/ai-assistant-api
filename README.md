# AI Assistant API

API REST construida con FastAPI que utiliza modelos LLM (Large Language Model) para responder preguntas sobre marketing digital y otros temas. Incluye endpoints para inicio de conversaciÃ³n, generaciÃ³n de respuestas y mÃ©tricas bÃ¡sicas de uso.

---
## Comparacion de modelos 

* **distilgpt2** es un modelo de lenguaje general preentrenado en inglÃ©s, optimizado para ser ligero y rÃ¡pido, pero no estÃ¡ especializado en espaÃ±ol ni en tareas especÃ­ficas de marketing digital. Su rendimiento en tareas en espaÃ±ol o contextos especÃ­ficos puede ser limitado, y tiende a generar respuestas menos relevantes o coherentes en otros idiomas, pero es una buena alternativa su idioma entrenado

* **datificate/gpt2-small-spanish** es una variante de GPT-2 entrenada especÃ­ficamente para el idioma espaÃ±ol. Esto le permite comprender mejor los matices, la gramÃ¡tica y el contexto cultural del espaÃ±ol, generando respuestas mÃ¡s adecuadas y naturales para usuarios, se logra una mayor cpherencia ajustando el prompt en las variables de entorno.

* Ambos modelos pueden ejecutarse en CPU, pero los modelos entrenados en espaÃ±ol, como datificate/gpt2-small-spanish , pueden requerir mÃ¡s recursos computacionales dependiendo de su tamaÃ±o y optimizaciÃ³n. En tus mÃ©tricas, se observa que el modelo en espaÃ±ol utiliza mÃ¡s CPU, aunque menos memoria en comparaciÃ³n con distilgpt2. (se pueden ver sus metricas en la API)

* La selecciÃ³n del modelo se puede ajustar fÃ¡cilmente desde las variables de entorno, permitiendo cambiar entre modelos sin modificar el cÃ³digo fuente. Para aplicaciones en espaÃ±ol o dominios especÃ­ficos, es preferible utilizar un modelo entrenado en ese idioma, como **datificate/gpt2-small-spanish** o **PlanTL-GOB-ES/gpt2-spanish**, si se requiere mayor calidad en las respuestas, se recomienda cargar un modelo mÃ¡s grande o especializado, siempre **considerando la capacidad de hardware disponible**.



## ğŸš€ CaracterÃ­sticas

- Carga y sirve modelos LLM con Hugging Face Transformers.
- API REST lista para producciÃ³n con documentaciÃ³n automÃ¡tica (Swagger UI).
- Soporta mÃºltiples modelos: phi-2, distilgpt2, gpt-neo, Mistral, Falcon, Llama, etc.
- MÃ©tricas de uso (latencia, tokens, nÃºmero de requests).
- Prompt pipeline para ingenierÃ­a de instrucciones bÃ¡sica.
- Ejemplo de uso CLI incluido.
- Docker listo para despliegue y compatibilidad multiplataforma.
- Historial de conversaciones persistente usando MongoDB, lo que permite almacenar, consultar y actualizar fÃ¡cilmente los mensajes de cada usuario o sesiÃ³n. 
---

## ğŸ“¦ Requisitos

- Python 3.10 o superior
- pip
- (Opcional) Docker
- MongoDB

---

## âš™ï¸ InstalaciÃ³n local

```bash
# Clona el repositorio y entra al directorio
git clone <tu-repo>
cd <carpeta-del-proyecto>

# Crea un entorno virtual (opcional)
python -m venv venv

# Activa el entorno virtual
# En Windows
.\venv\Scripts\Activate.ps1 (recomendado)

# Instala dependencias
pip install -r requirements.txt

# Ejecuta el servidor FastAPI
uvicorn app.main:app --reload
```



## Carga en GitHub Codespaces

1. Abre el repositorio en GitHub y haz clic en el botÃ³n **"Code"** > **"Open with Codespaces"** > **"New codespace"**.
2. Espera a que Codespaces configure el entorno automÃ¡ticamente.
3. Instala las dependencias (si no se instalan automÃ¡ticamente):

```bash

# Instala dependencias
pip install -r requirements.txt

# Ejecuta el servidor FastAPI
python -m uvicorn app.main:app --reload

```

En GitHub Codespaces, el servidor se ejecuta dentro de un contenedor remoto, asÃ­ que no puedes acceder directamente a http://127.0.0.1:8000 desde tu navegador local. Debes usar el reenvÃ­o de puertos que ofrece Codespaces.

1.- Cuando corres el servidor, Codespaces detecta que el puerto 8000 estÃ¡ en uso y te muestra un botÃ³n o enlace en la parte inferior o superior de VS Code (o en la interfaz web) para abrir el puerto en el navegador.
2.- Haz clic en ese enlace (usualmente dice algo como â€œOpen in Browserâ€ o â€œAbrir en navegadorâ€).
3.- Se abrirÃ¡ una URL similar a:

https://<tu-usuario>-<id>-8000.app.github.dev/docs

**Â¡Agrega /docs al final si no aparece automÃ¡ticamente!**



---

## ğŸ› ï¸ Variables de entorno

Configura tus variables de entorno en un archivo `.env` o en `app/settings/general_settings.py`:

- `MODEL`: Nombre del modelo Hugging Face a usar (ejemplo: `datificate/gpt2-small-spanish`)
- `SYSTEM_PROMPT`: Prompt inicial del sistema para las respuestas del asistente
- `TEMPERATURE`: Temperatura para la generaciÃ³n de texto (controla la creatividad, valor recomendado: 0.7)
- `MAX_LENGHT`: Longitud mÃ¡xima de la respuesta generada (por ejemplo: 150)
- `TASK`: Tarea del pipeline de transformers (por ejemplo, `text-generation`)
- `MONGO_URI`: URI de conexiÃ³n a MongoDB (ejemplo: `mongodb://localhost:27017/`)
- `COLLECTION_HISTORY`: Nombre de la colecciÃ³n para historial de conversaciones (ejemplo: `conversations`)
- `BASE_NAME`: Nombre de la base de datos en MongoDB (ejemplo: `ai_assistant_db`)

---

## ğŸ“š Uso

Una vez iniciado el servidor, accede a la documentaciÃ³n interactiva en [http://localhost:8000/docs](http://localhost:8000/docs).

### Ejemplo de endpoints principales

- **Iniciar conversaciÃ³n**
  ```
  POST /start_conversation
  ```

- **Generar respuesta**
  ```
  POST /generate
  ```

- **Obtener mÃ©tricas**
  ```
  GET /metrics
  ```

---

## ğŸ“ Estructura del proyecto

```
app/
â”œâ”€â”€ __pycache__/
â”œâ”€â”€ env/
â”‚   â””â”€â”€ general_settings.env
â”œâ”€â”€ settings/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ general_settings.py
â”œâ”€â”€ main.py
â”œâ”€â”€ model.py
â”œâ”€â”€ metrics_logger.py
â”œâ”€â”€ schemas.py
venv/
Dockerfile
README.md
requirements.txt

```

---

## ğŸ³ Docker

Puedes construir y correr el proyecto con Docker:

```bash
docker build -t ai-assistant-api .
docker run -p 8000:8000 --env-file .env ai-assistant-api
```

---

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Consulta el archivo [LICENSE](LICENSE) para mÃ¡s informaciÃ³n.